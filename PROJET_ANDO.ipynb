{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f036080c",
   "metadata": {},
   "source": [
    "# ANDO: NEW YORK AIRBNB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aae251",
   "metadata": {},
   "source": [
    "## Importation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fc14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn pour la préparation, la réduction de dimension et le clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\t\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7021dc",
   "metadata": {},
   "source": [
    "## Lecture du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AB_NYC_2019.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f02f4",
   "metadata": {},
   "source": [
    "## Tache 1: Nettoyage et preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c88b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remplacer les valeurs manquantes dans reviews_per_month\n",
    "df['reviews_per_month'].fillna(0, inplace=True)\n",
    "print(\"Les valeurs manquantes de reviews_per_month ont été remplacées par 0.\")\n",
    "\n",
    "# 2. Supprimer les colonnes inutiles\n",
    "to_drop = ['name', 'id', 'last_review']\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "print(\"Les colonnes name, id et last_review ont été supprimées.\")\n",
    "\n",
    "# 3. Supprimer les valeurs extrêmes dans price avec le 99e percentile\n",
    "price_cap = df['price'].quantile(0.99)\n",
    "df = df[df['price'] < price_cap]\n",
    "df = df[df['price'] > 0]\n",
    "print(f\"Les valeurs extrêmes de price au-dessus de {price_cap} ont été retirées.\")\n",
    "\n",
    "# 4. Créer une nouvelle variable log_price\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "df['log_price'] = df['log_price'].round(2)\n",
    "print(\"La variable log_price a été créée à partir de price.\")\n",
    "\n",
    "# 5. Créer les types d'hôtes selon le nombre d'annonces\n",
    "df['host_type'] = 0\n",
    "df.loc[df['calculated_host_listings_count'] == 1, 'host_type'] = 0        # Occasionnel\n",
    "df.loc[(df['calculated_host_listings_count'] >= 2) \n",
    "       & (df['calculated_host_listings_count'] <= 5), 'host_type'] = 1    # Multi\n",
    "df.loc[df['calculated_host_listings_count'] > 5, 'host_type'] = 2         # Opérateur\n",
    "\n",
    "print(\"La variable host_type a été créée selon le volume d'annonces.\")\n",
    "\n",
    "# 6. Copier le dataset final\n",
    "df_cleaned = df.copy()\n",
    "print(\"Nettoyage terminé. Le DataFrame propre est prêt pour l'analyse.\")\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a774d",
   "metadata": {},
   "source": [
    "## Tache 2:  ANALYSE EXPLORATOIRE ET TESTS D'HYPOTHÈSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e87b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 general info\n",
    "print(\"DataFrame Info:\")\n",
    "display(df_cleaned.info())\n",
    "\n",
    "# 2 missing values\n",
    "print(\"\\nMissing percentile Values in Each Column:\")\n",
    "display(df_cleaned.isnull().sum()/df_cleaned.shape[0] * 100)\n",
    "\n",
    "# 3 statistical summary\n",
    "print(\"\\nStatistical Summary of Numerical Columns:\")\n",
    "display(df_cleaned.describe())\n",
    "\n",
    "# statistical summary categorical\n",
    "display(df.describe(exclude=['number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer host_name \n",
    "df['host_name'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec809af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 price distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_cleaned['price'], kde=True, bins=50)\n",
    "plt.title('Distribution des Prix (Brute)')\n",
    "# 2 log_price distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_cleaned['log_price'], kde=True, bins=50)\n",
    "plt.title('Distribution du Log-Prix (plus proche de la loi Normale)')\n",
    "plt.show()\n",
    "\n",
    "#  3 Bivariate Analysis: Boxplot of log_price by neighbourhood_group\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df_cleaned, x='neighbourhood_group', y='log_price')\n",
    "plt.title('Boxplot du Log-Prix par Arrondissement')\n",
    "plt.show()\n",
    "\n",
    "# 4 Bivariate Analysis: Boxplot of log_price by room_type\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df_cleaned, x='room_type', y='log_price')\n",
    "plt.title('Boxplot du Log-Prix par type de logement')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 5 collation matrix\n",
    "numerical_cols = ['log_price', 'minimum_nights', 'number_of_reviews', \n",
    "                  'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'host_type']\n",
    "corr_matrix = df_cleaned[numerical_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87622a",
   "metadata": {},
   "source": [
    "### Teste d'hypothese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing: ANOVA test for log_price across neighbourhood_group\n",
    "# 1 ANOVA test\n",
    "groups = [group['log_price'].values for name, group in df_cleaned.groupby('neighbourhood_group')]\n",
    "f_stat, p_value = stats.f_oneway(*groups)\n",
    "print(f\"ANOVA test results for log_price across neighbourhood_group: F-statistic = {f_stat}, p-value = {p_value}\")\n",
    "if p_value < 0.05:\n",
    "\tprint(\"Reject the null hypothesis: Significant differences exist between groups.\")\t\n",
    "else:\n",
    "\tprint(\"Fail to reject the null hypothesis: No significant differences between groups.\")\n",
    " \n",
    " # 2 t-test for log_price between two room_types: 'Shared room' and 'Private room'\n",
    "entire_home = df_cleaned[df_cleaned['room_type'] == 'Shared room']['log_price']\n",
    "private_room = df_cleaned[df_cleaned['room_type'] == 'Private room']['log_price']\n",
    "t_stat, p_value = stats.ttest_ind(entire_home, private_room, equal_var=False)\n",
    "print(f\"T-test results for log_price between 'Shared room' and 'Private room': T-statistic = {t_stat}, p-value = {p_value}\")\n",
    "if p_value < 0.05:\t\t\n",
    "\tprint(\"Reject the null hypothesis: Significant difference in log_price between the two room types.\")\t\n",
    "else:\n",
    "\tprint(\"Fail to reject the null hypothesis: No significant difference in log_price between the two room types.\")\n",
    "\n",
    "# 3 Chi-squared test for independence between neighbourhood_group and room_type\n",
    "contingency_table = pd.crosstab(df_cleaned['neighbourhood_group'], df_cleaned['room_type'])\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f\"Chi-squared test results between neighbourhood_group and room_type: Chi2-statistic = {chi2_stat}, p-value = {p_value}\")\n",
    "if p_value < 0.05:\n",
    "\tprint(\"Reject the null hypothesis: Variables are dependent.\")\t\n",
    "else:\n",
    "\tprint(\"Fail to reject the null hypothesis: Variables are independent.\")\n",
    " \n",
    "# 4. Tests d'Hypothèses\n",
    "# Test 1: T-test de Student - Le prix moyen à Manhattan est-il différent de celui de Brooklyn ?\n",
    "manhattan_prices = df_cleaned[df_cleaned['neighbourhood_group'] == 'Queens']['log_price']\n",
    "brooklyn_prices = df_cleaned[df_cleaned['neighbourhood_group'] == 'Brooklyn']['log_price']\n",
    "t_stat, p_value = stats.ttest_ind(manhattan_prices, brooklyn_prices, equal_var=False)\n",
    "print(f\"--- T-test (Manhattan vs Brooklyn) ---\")\n",
    "print(f\"P-valeur: {p_value:.5f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Conclusion : On rejette l'hypothèse nulle. Les prix moyens sont significativement différents.\")\n",
    "else:\n",
    "    print(\"Conclusion : On ne peut pas rejeter l'hypothèse nulle.\") \n",
    "\n",
    "# 5 Test 2: Test du Chi-carré - Y a-t-il une dépendance entre le type de chambre et l'arrondissement ?\n",
    "contingency_table = pd.crosstab(df_cleaned['neighbourhood_group'], df_cleaned['room_type'])\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "print(f\"\\n--- Test du Chi-carré (Arrondissement vs Type de chambre) ---\")\n",
    "print(f\"P-valeur: {p_value:.5f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Conclusion : On rejette l'hypothèse nulle. Il y a une association significative entre les variables.\")\n",
    "else:\n",
    "    print(\"Conclusion : On ne peut pas rejeter l'hypothèse nulle.\")\n",
    "display(contingency_table)\n",
    "\n",
    "# Post-hoc analysis: Tukey's HSD test for neighbourhood_group \n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "tukey = pairwise_tukeyhsd(df_cleaned['log_price'], df_cleaned['neighbourhood_group'])\n",
    "print(tukey)\n",
    "\n",
    "# Post-hoc analysis: Tukey's HSD test for room_type \n",
    "tukey_room = pairwise_tukeyhsd(df_cleaned['log_price'], df_cleaned['room_type'])\n",
    "print(tukey_room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_room_type_neighbourhood = df.room_type.groupby(df.neighbourhood_group).value_counts().unstack()\n",
    "df_room_type_neighbourhood.plot(kind='bar', stacked=True, figsize=(10,6))\n",
    "plt.title('Répartition des types de logement par arrondissement')\n",
    "plt.xlabel('Arrondissement')\n",
    "plt.ylabel('Nombre de logements')\n",
    "plt.legend(title='Type de logement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6d68d",
   "metadata": {},
   "source": [
    "## Tache 3: PCA et CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ca0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTIE 4 : RÉDUCTION DE DIMENSION (Leçons 3-4-5) - CORRIGÉ #2\n",
    "print(\"\\n--- Étape 4: Réduction de Dimension ---\")\n",
    "\n",
    "# 1. Analyse en Composantes Principales (ACP) sur les données numériques\n",
    "print(\"--- Lancement de l'ACP ---\")\n",
    "features_for_pca = ['log_price', 'minimum_nights', 'number_of_reviews',\n",
    "                      'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
    "\n",
    "# Centrer et réduire les données est CRUCIAL pour l'ACP\n",
    "X = df_cleaned[features_for_pca].values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Application de l'ACP\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Ajout des informations qualitatives pour la visualisation\n",
    "df_pca.index = df_cleaned.index\n",
    "df_pca['host_type'] = df_cleaned['host_type']\n",
    "\n",
    "# Visualisation des individus sur le premier plan factoriel\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='host_type', alpha=0.5)\n",
    "plt.title('Projection des Annonces sur les 2 Premières Composantes Principales (ACP)')\n",
    "plt.xlabel(f'PC1 - {pca.explained_variance_ratio_[0]*100:.2f}% variance')\n",
    "plt.ylabel(f'PC2 - {pca.explained_variance_ratio_[1]*100:.2f}% variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Cercle des corrélations (Interprétation des axes)\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for i, feature in enumerate(features_for_pca):\n",
    "    ax.arrow(0, 0, loadings[i,0], loadings[i,1], color='r', alpha=0.8, head_width=0.02)\n",
    "    ax.text(loadings[i,0]*1.15, loadings[i,1]*1.15, feature, color='black')\n",
    "circle = plt.Circle((0,0), 1, color='gray', fill=False, linestyle='--')\n",
    "ax.add_artist(circle)\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('Cercle des Corrélations')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Analyse des Correspondances Multiples (ACM) sur les données catégorielles\n",
    "print(\"\\n--- Lancement de l'ACM ---\")\n",
    "df_cleaned['price_category'] = pd.qcut(df_cleaned['price'], q=3, labels=['Bas', 'Moyen', 'Élevé'])\n",
    "features_for_mca = ['neighbourhood_group', 'room_type', 'price_category']\n",
    "df_mca_input = df_cleaned[features_for_mca]\n",
    "\n",
    "'''mca = prince.MCA(n_components=2, n_iter=3, random_state=42)\n",
    "mca = mca.fit(df_mca_input)\n",
    "\n",
    "# CORRECTION #2 : La librairie prince utilise Altair. La syntaxe pour le titre change.\n",
    "chart = mca.plot(df_mca_input)\n",
    "chart = chart.properties(\n",
    "    title='Projection des Modalités sur le Premier Plan Factoriel (ACM)'\n",
    ")\n",
    "ax = mca.plot(df_mca_input)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af435ac4",
   "metadata": {},
   "source": [
    "## Tâche 4: Clustering et segmentation des hôtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARTIE 5 : CLUSTERING (Leçon 6)\n",
    "print(\"\\n--- Étape 5: Clustering ---\")\n",
    "\n",
    "# 1. K-Means sur les composantes principales de l'ACP\n",
    "print(\"--- Lancement du K-Means ---\")\n",
    "# On utilise les données de l'ACP pour le clustering\n",
    "X_for_kmeans = df_pca[['PC1', 'PC2']].values\n",
    "\n",
    "# Détermination du nombre optimal de clusters (Méthode du coude)\n",
    "inertia = []\n",
    "K_range = range(2, 10)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_for_kmeans)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertia, 'bo-')\n",
    "plt.xlabel('Nombre de clusters (k)')\n",
    "plt.ylabel('Inertie')\n",
    "plt.title('Méthode du Coude pour déterminer k optimal')\n",
    "plt.show()\n",
    "\n",
    "# On choisit k=4 (un coude visible)\n",
    "k_optimal = 3\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "df_pca['kmeans_cluster'] = kmeans.fit_predict(X_for_kmeans)\n",
    "print(f\"Clustering K-Means avec k={k_optimal} terminé.\")\n",
    "\n",
    "# Visualisation des clusters K-Means\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='kmeans_cluster', palette='viridis', alpha=0.7)\n",
    "plt.title('Clusters K-Means visualisés sur le plan de l\\'ACP')\n",
    "plt.show()\n",
    "\n",
    "# Interprétation des clusters\n",
    "cluster_profile = df_cleaned.join(df_pca['kmeans_cluster'])\n",
    "print(\"Profil moyen des clusters (sur données non-standardisées) :\")\n",
    "print(cluster_profile.groupby('kmeans_cluster')[numerical_cols].mean())\n",
    "\n",
    "\n",
    "# 2. DBSCAN pour le clustering spatial\n",
    "print(\"\\n--- Lancement du DBSCAN ---\")\n",
    "# On prend un échantillon pour des raisons de performance\n",
    "df_sample_spatial = df_cleaned.sample(n=20000, random_state=42)\n",
    "coords = df_sample_spatial[['latitude', 'longitude']].values\n",
    "\n",
    "# Les paramètres eps et min_samples sont cruciaux et nécessitent des tests.\n",
    "# eps=0.01 (environ 1km à NYC), min_samples=50\n",
    "dbscan = DBSCAN(eps=0.01, min_samples=50)\n",
    "df_sample_spatial['dbscan_cluster'] = dbscan.fit_predict(coords)\n",
    "print(\"Clustering spatial DBSCAN terminé.\")\n",
    "print(f\"Nombre de clusters trouvés : {df_sample_spatial['dbscan_cluster'].nunique() - 1}\")\n",
    "print(f\"Nombre de points 'bruit' (outliers) : {(df_sample_spatial['dbscan_cluster'] == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aab05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "# Visualisation des clusters DBSCAN\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.scatterplot(data=df_sample_spatial, x='longitude', y='latitude', \n",
    "                hue='host_type', palette='deep', s=10, \n",
    "                legend='full')\n",
    "plt.title('Clusters Spatiaux identifiés par DBSCAN')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend(title='Cluster ID')\n",
    "plt.show()\n",
    "\n",
    "gdf_spatial = gpd.GeoDataFrame(\n",
    "    df_sample_spatial,\n",
    "    geometry=gpd.points_from_xy(df_sample_spatial.longitude, df_sample_spatial.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "gdf_spatial.plot(\n",
    "    ax=ax,\n",
    "    column='dbscan_cluster',\n",
    "    categorical=True,\n",
    "    cmap='viridis',\n",
    "    markersize= 5,\n",
    "    alpha=0.7,\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "ax.set_axis_off()\n",
    "plt.title('DBSCAN Spatial Clusters on NYC Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa6191",
   "metadata": {},
   "source": [
    "## Tâche 5. Analyse spatiale et modèle explicatif\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
